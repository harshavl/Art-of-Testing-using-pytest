{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytest was created to make testing easier, more Pythonic,\n",
    "and extensible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TDD<br>\n",
    "• Test-driven development (TDD) is a popular technique.<br>\n",
    "• Write the test for a small, new piece of functionality. It’ll fail.<br>\n",
    "• Write the code to make sure the test passes.<br>\n",
    "• Run the test. See it passes. Go back to the first step.<br>\n",
    "• pytest can be a part of a TDD methodology. But it won't tell you\n",
    "that you must.\n",
    "• pytest ignores or not support \"type annotations\". So other tools like \"mypy\".<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Linux, please set the env set up for the directory and use the **__init__.py** in the directory. so that, can access files at any where."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hello.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hello.py\n",
    "\n",
    "def speak(name):\n",
    "    return f\"Hello,{name}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_hello.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_hello.py\n",
    "\n",
    "from hello import speak\n",
    "\n",
    "def test_speak():\n",
    "    output = speak(\"Harsha\")\n",
    "    assert output == \"Hello,Harsha\"\n",
    "    \n",
    "    \n",
    "def test_speak_1():\n",
    "    output = speak(\"harsha\")\n",
    "    assert \"harsha\" in output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.3, pytest-6.0.2, py-1.9.0, pluggy-0.13.1 -- c:\\users\\hl3\\anaconda3\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: C:\\Users\\hl3\\OneDrive - DXC Production\\heap\\unit_testing_software_pytest\\pytest\n",
      "collecting ... collected 2 items\n",
      "\n",
      "test_hello.py::test_speak PASSED                                         [ 50%]\n",
      "test_hello.py::test_speak_1 PASSED                                       [100%]\n",
      "\n",
      "============================== 2 passed in 0.16s ==============================\n"
     ]
    }
   ],
   "source": [
    "! pytest -vv test_hello.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example 2: Add numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mysum.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mysum.py\n",
    "\n",
    "def mysum(numbers):\n",
    "    total = 0\n",
    "    for number in numbers:\n",
    "        total += number\n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_mysum.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_mysum.py\n",
    "\n",
    "from mysum import mysum\n",
    "\n",
    "    \n",
    "    \n",
    "def test_mysum_list():\n",
    "    assert mysum([10,20,30] ) == 60\n",
    "    assert mysum([10.2, 20, 30 ]) == 60.2 \n",
    "    \n",
    "def test_mysum_float():\n",
    "    assert mysum([10.2, 20, 30 ]) == 60.2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.3, pytest-6.0.2, py-1.9.0, pluggy-0.13.1 -- c:\\users\\hl3\\anaconda3\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: C:\\Users\\hl3\\OneDrive - DXC Production\\heap\\unit_testing_software_pytest\\pytest\n",
      "plugins: sugar-0.9.4\n",
      "collecting ... collected 2 items\n",
      "\n",
      "test_mysum.py::test_mysum_list PASSED                                    [ 50%]\n",
      "test_mysum.py::test_mysum_float PASSED                                   [100%]\n",
      "\n",
      "============================== 2 passed in 0.18s ==============================\n"
     ]
    }
   ],
   "source": [
    "! pytest -vv test_mysum.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytest-sugar\n",
      "  Downloading pytest-sugar-0.9.4.tar.gz (12 kB)\n",
      "Requirement already satisfied, skipping upgrade: pytest>=2.9 in c:\\users\\hl3\\anaconda3\\lib\\site-packages (from pytest-sugar) (6.0.2)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=14.1 in c:\\users\\hl3\\anaconda3\\lib\\site-packages (from pytest-sugar) (20.4)\n",
      "Requirement already satisfied, skipping upgrade: colorama; sys_platform == \"win32\" in c:\\users\\hl3\\anaconda3\\lib\\site-packages (from pytest>=2.9->pytest-sugar) (0.4.3)\n",
      "Requirement already satisfied, skipping upgrade: iniconfig in c:\\users\\hl3\\anaconda3\\lib\\site-packages (from pytest>=2.9->pytest-sugar) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pluggy<1.0,>=0.12 in c:\\users\\hl3\\anaconda3\\lib\\site-packages (from pytest>=2.9->pytest-sugar) (0.13.1)\n",
      "Requirement already satisfied, skipping upgrade: py>=1.8.2 in c:\\users\\hl3\\anaconda3\\lib\\site-packages (from pytest>=2.9->pytest-sugar) (1.9.0)\n",
      "Requirement already satisfied, skipping upgrade: toml in c:\\users\\hl3\\anaconda3\\lib\\site-packages (from pytest>=2.9->pytest-sugar) (0.10.1)\n",
      "Requirement already satisfied, skipping upgrade: atomicwrites>=1.0; sys_platform == \"win32\" in c:\\users\\hl3\\anaconda3\\lib\\site-packages (from pytest>=2.9->pytest-sugar) (1.4.0)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools>=4.0.0 in c:\\users\\hl3\\anaconda3\\lib\\site-packages (from pytest>=2.9->pytest-sugar) (8.4.0)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in c:\\users\\hl3\\anaconda3\\lib\\site-packages (from pytest>=2.9->pytest-sugar) (19.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in c:\\users\\hl3\\anaconda3\\lib\\site-packages (from packaging>=14.1->pytest-sugar) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: six in c:\\users\\hl3\\anaconda3\\lib\\site-packages (from packaging>=14.1->pytest-sugar) (1.15.0)\n",
      "Building wheels for collected packages: pytest-sugar, termcolor\n",
      "  Building wheel for pytest-sugar (setup.py): started\n",
      "  Building wheel for pytest-sugar (setup.py): finished with status 'done'\n",
      "  Created wheel for pytest-sugar: filename=pytest_sugar-0.9.4-py2.py3-none-any.whl size=8977 sha256=a4f21c1b2e29f059f107ce15ff25342fd02f9b2052ed36de8b9cb38f8f61ad06\n",
      "  Stored in directory: c:\\users\\hl3\\appdata\\local\\pip\\cache\\wheels\\03\\4e\\79\\49721a5712613cdb0fc9d9e09aba8bdf42774c22808e8bca69\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4835 sha256=4bdd3ac450f919d517ea3e6f8b61f80216503e8cc856126e7da12d355ec4bad1\n",
      "  Stored in directory: c:\\users\\hl3\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built pytest-sugar termcolor\n",
      "Installing collected packages: termcolor, pytest-sugar\n",
      "Successfully installed pytest-sugar-0.9.4 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "! pip install -U pytest-sugar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Assertions<br>\n",
    "\n",
    "assert x is None <br>\n",
    "assert x == 10<br>\n",
    "assert abs(x-10) < 3<br>\n",
    "assert ‘abc’ in returned_value<br>\n",
    "assert x in [10, 20, 30]<br>\n",
    "assert returned_value.startswith(‘abc’)<br>\n",
    "\n",
    "assert len(x) == 5<br>\n",
    "assert x == {10, 20, 30, 40}<br>\n",
    "assert set(x) == set(y)<br>\n",
    "assert type(x) is str<br>\n",
    "assert iter(x) is x<br>\n",
    "assert keys(x) == keys(y)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing mulitple tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.3, pytest-6.0.2, py-1.9.0, pluggy-0.13.1 -- c:\\users\\hl3\\anaconda3\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: C:\\Users\\hl3\\OneDrive - DXC Production\\heap\\unit_testing_software_pytest\\pytest\n",
      "plugins: sugar-0.9.4\n",
      "collecting ... collected 1 item\n",
      "\n",
      "test_mysum.py::test_mysum_float PASSED                                   [100%]\n",
      "\n",
      "============================== 1 passed in 0.09s ==============================\n"
     ]
    }
   ],
   "source": [
    "! pytest -vv test_mysum.py::test_mysum_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.3, pytest-6.0.2, py-1.9.0, pluggy-0.13.1\n",
      "rootdir: C:\\Users\\hl3\\OneDrive - DXC Production\\heap\\unit_testing_software_pytest\\pytest\n",
      "plugins: sugar-0.9.4\n",
      "collected 2 items / 1 deselected / 1 selected\n",
      "\n",
      "test_mysum.py .                                                          [100%]\n",
      "\n",
      "======================= 1 passed, 1 deselected in 0.06s =======================\n"
     ]
    }
   ],
   "source": [
    "! pytest test_mysum.py -k test_mysum_float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Complex expressions<br>\n",
    "• The argument to the -k option are treated as substrings<br>\n",
    "• You can use “and”, “or”, and “not” to choose which tests\n",
    "to run<br>\n",
    "\n",
    "for examples, <br>\n",
    "pytest test_mysum.py -k 'not mysum_returns_a’ <br>\n",
    "pytest test_mysum.py -k 'int or float'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.3, pytest-6.0.2, py-1.9.0, pluggy-0.13.1\n",
      "rootdir: C:\\Users\\hl3\\OneDrive - DXC Production\\heap\\unit_testing_software_pytest\\pytest\n",
      "plugins: sugar-0.9.4\n",
      "collected 4 items\n",
      "\n",
      "test_hello.py ..                                                         [ 50%]\n",
      "test_mysum.py ..                                                         [100%]\n",
      "\n",
      "============================== 4 passed in 0.28s ==============================\n"
     ]
    }
   ],
   "source": [
    "! pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naming test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sort.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sort.py\n",
    "\n",
    "def sorted_unique_ints(numbers):\n",
    "    return sorted( { number for number in numbers } )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_sort.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_sort.py\n",
    "\n",
    "from sort import sorted_unique_ints\n",
    "\n",
    "def test_uniqueness():\n",
    "    results = sorted_unique_ints( [ 10, 30, 20, 10, 15 , 40 ] )\n",
    "    assert set(results) == {10,15,20,30}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.3, pytest-6.0.2, py-1.9.0, pluggy-0.13.1 -- c:\\users\\hl3\\anaconda3\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: C:\\Users\\hl3\\OneDrive - DXC Production\\heap\\unit_testing_software_pytest\\pytest\n",
      "plugins: sugar-0.9.4\n",
      "collecting ... collected 1 item\n",
      "\n",
      "test_sort.py::test_uniqueness FAILED                                     [100%]\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "_______________________________ test_uniqueness _______________________________\n",
      "\n",
      "    def test_uniqueness():\n",
      "        results = sorted_unique_ints( [ 10, 30, 20, 10, 15 , 40 ] )\n",
      ">       assert set(results) == {10,15,20,30}\n",
      "E       assert {40, 10, 15, 20, 30} == {10, 20, 30, 15}\n",
      "E         Extra items in the left set:\n",
      "E         40\n",
      "E         Full diff:\n",
      "E         - {10, 20, 30, 15}\n",
      "E         + {40, 10, 15, 20, 30}\n",
      "\n",
      "test_sort.py:6: AssertionError\n",
      "=========================== short test summary info ===========================\n",
      "FAILED test_sort.py::test_uniqueness - assert {40, 10, 15, 20, 30} == {10, 20...\n",
      "============================== 1 failed in 0.36s ==============================\n"
     ]
    }
   ],
   "source": [
    "! pytest -vv test_sort.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: Pytest shows us where the problem was.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.3, pytest-6.0.2, py-1.9.0, pluggy-0.13.1\n",
      "rootdir: C:\\Users\\hl3\\OneDrive - DXC Production\\heap\\unit_testing_software_pytest\\pytest\n",
      "plugins: sugar-0.9.4\n",
      "collected 1 item\n",
      "\n",
      "test_sort.py F\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "_______________________________ test_uniqueness _______________________________\n",
      "\n",
      "    def test_uniqueness():\n",
      "        results = sorted_unique_ints( [ 10, 30, 20, 10, 15 , 40 ] )\n",
      ">       assert set(results) == {10,15,20,30}\n",
      "E       assert {10, 15, 20, 30, 40} == {10, 15, 20, 30}\n",
      "E         Extra items in the left set:\n",
      "E         40\n",
      "E         Use -v to get the full diff\n",
      "\n",
      "test_sort.py:6: AssertionError\n",
      "=========================== short test summary info ===========================\n",
      "FAILED test_sort.py::test_uniqueness - assert {10, 15, 20, 30, 40} == {10, 15...\n",
      "============================== 1 failed in 0.19s ==============================\n"
     ]
    }
   ],
   "source": [
    "! pytest -s test_sort.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.3, pytest-6.0.2, py-1.9.0, pluggy-0.13.1\n",
      "rootdir: C:\\Users\\hl3\\OneDrive - DXC Production\\heap\\unit_testing_software_pytest\\pytest\n",
      "plugins: sugar-0.9.4\n",
      "collected 1 item\n",
      "\n",
      "test_sort.py F                                                           [100%]\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "_______________________________ test_uniqueness _______________________________\n",
      "\n",
      "    def test_uniqueness():\n",
      "        results = sorted_unique_ints( [ 10, 30, 20, 10, 15 , 40 ] )\n",
      ">       assert set(results) == {10,15,20,30}\n",
      "E       assert {10, 15, 20, 30, 40} == {10, 15, 20, 30}\n",
      "E         Extra items in the left set:\n",
      "E         40\n",
      "E         Use -v to get the full diff\n",
      "\n",
      "test_sort.py:6: AssertionError\n",
      "=========================== short test summary info ===========================\n",
      "FAILED test_sort.py::test_uniqueness - assert {10, 15, 20, 30, 40} == {10, 15...\n",
      "============================== 1 failed in 0.19s ==============================\n"
     ]
    }
   ],
   "source": [
    "! pytest --capture=sys test_sort.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exit on first error with -x<br>\n",
    "\n",
    "you want to deal with the errors one by\n",
    "one. For that, you have -x<br>\n",
    "It means: Run the tests until/unless you encounter an\n",
    "error. At which point, stop.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.3, pytest-6.0.2, py-1.9.0, pluggy-0.13.1\n",
      "rootdir: C:\\Users\\hl3\\OneDrive - DXC Production\\heap\\unit_testing_software_pytest\\pytest\n",
      "plugins: sugar-0.9.4\n",
      "collected 5 items\n",
      "\n",
      "test_hello.py ..                                                         [ 40%]\n",
      "test_mysum.py ..                                                         [ 80%]\n",
      "test_sort.py F                                                           [100%]\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "_______________________________ test_uniqueness _______________________________\n",
      "\n",
      "    def test_uniqueness():\n",
      "        results = sorted_unique_ints( [ 10, 30, 20, 10, 15 , 40 ] )\n",
      ">       assert set(results) == {10,15,20,30}\n",
      "E       assert {10, 15, 20, 30, 40} == {10, 15, 20, 30}\n",
      "E         Extra items in the left set:\n",
      "E         40\n",
      "E         Use -v to get the full diff\n",
      "\n",
      "test_sort.py:6: AssertionError\n",
      "=========================== short test summary info ===========================\n",
      "FAILED test_sort.py::test_uniqueness - assert {10, 15, 20, 30, 40} == {10, 15...\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!! stopping after 1 failures !!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "========================= 1 failed, 4 passed in 0.25s =========================\n"
     ]
    }
   ],
   "source": [
    "! pytest  -x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.8.3, pytest-6.0.2, py-1.9.0, pluggy-0.13.1\n",
      "rootdir: C:\\Users\\hl3\\OneDrive - DXC Production\\heap\\unit_testing_software_pytest\\pytest\n",
      "plugins: sugar-0.9.4\n",
      "collected 1 item\n",
      "\n",
      "test_sort.py F                                                           [100%]\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "_______________________________ test_uniqueness _______________________________\n",
      "\n",
      "    def test_uniqueness():\n",
      "        results = sorted_unique_ints( [ 10, 30, 20, 10, 15 , 40 ] )\n",
      ">       assert set(results) == {10,15,20,30}\n",
      "E       assert {10, 15, 20, 30, 40} == {10, 15, 20, 30}\n",
      "E         Extra items in the left set:\n",
      "E         40\n",
      "E         Use -v to get the full diff\n",
      "\n",
      "test_sort.py:6: AssertionError\n",
      "=========================== short test summary info ===========================\n",
      "FAILED test_sort.py::test_uniqueness - assert {10, 15, 20, 30, 40} == {10, 15...\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!! stopping after 1 failures !!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "============================== 1 failed in 0.20s ==============================\n"
     ]
    }
   ],
   "source": [
    "! pytest  test_sort.py -x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## maxfail<br>\n",
    "\n",
    "• The -x option tells pytest to stop after the first error.<br>\n",
    "\n",
    "• The --maxfail option lets you specify any number, after\n",
    "which pytest should stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
